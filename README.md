# Microsoft_Malware_Detection
#**BLUF**
**Team 28ï¼Œ Members** \
**member 1**: Yifan Lin \
**member 2**: Muhao Chen

**1. Get & Load Data** \
* The dataset is Microsoft Malware Prediction \
https://www.kaggle.com/competitions/microsoft-malware-prediction/data
* Because the original data is extremely large, we sample randomly 3000 items from Kaggle. The final Size in our project is 3306 kb (3,000 items * 83 columns) The dataset is balanced, and it is a binary classification project.

**2. Exploratory Data Analysis (EDA)** \
*Data preprocessing requirements are as follows:* 
* this is a binary classification problem, the taget is *HasDetections*, 1 means the Malware is detected in the machine, 0 means there is no Malware in the machine. The number of 1 is 1515, the number of 0 is 1485.
* "ProductName", "MachineIdentifier" and other 52 features may be useless, or have too many missing value, can be deleted.
* we have 23 categorical features, 7 quantitative features and 1 target.
* we encoding "Platform", "Processor" and other 8 features, using label encoder.
* we use 0 value and mean value handle the missing value.
* There is no duplicate value.
* We perform descriptive statistics on numerical variables, including box plots, distribution plots, and violin plots.
* We perform descriptive statistics on categorical variables, including crosstabs and distribution plots.
* We study correlations between numerical variables through heatmaps and correlation distribution plots.


*Relationship insights are as follows:*
* while *Processor* is 1, Malware is difficult to appear. while *Census_HasOpticalDiskDrive* is 1, Malware is easy to appear.
* *Census_TotalPhysicalRAM* will likely have higher value if there is no Malware. *Census_SystemVolumeTotalCapacity* will likely have higher value if there is Malware.
* *Census_InternalPrimaryDiagonalDisplaySizeInInches* seems have some outliers.
* *Census_InternalPrimaryDiagonalDisplaySizeInInches* and *Census_TotalPhysicalRAM* are correlated with *HasDetections*.

**3. Data Preprocessing** \
*3.1 Process*\
1.We extracted the predictors and understood the shape of this dataframe
2. Then we checked for skewness, and corrected any outlier with turkey rules 
3. Then we utiliizing pandas .value_counts to check if the data is balance. We found out that there are 1515 has_detection and 1485 not_has_detection. So we could concluded that the data is balance (50.5%)
4. Then we conducted the PCA to compare the strength of each predictor as shown in the pairplot. And we also checked the interactions between each predictor(Principal Components) with seaborn
*3.2 Conclusion*\
The data is balanced(50.5%) and we dont have to use SMOTH to balance the data. We also understood the strength of each predictor.

**4. Classification (ML based)** \
*4.1 Process* \
1. We use our code, ["Perceptron", "Logistic Regression", "SVM (RBF kernel)", "Decision Tree", "Naive Bayes", "k Nearest Neighbors", "Random Forest", "XG Boost", "Light GBM"] to roughly get prediction result.
2. We ues low code, roughly get prediction result.
3. Then we splitted the data to training and testing dataset in the ration of 80:20.
4. We performed logistic regression machine learning algorithm and received the result in the following: AUC is around 0.527, recall is around 0.644, and the accuracy is 0.528. Percentage of test instances with propensity in [0.45, 0.55]: 68.33. 
5. Then we established the XGBoost machine learning algorithm using grid search to find the best model based on different hyperparameter. The result shows the validation mean accuracy is 0.58125. In test data, we gets the accuracy of 0.57, the AUC is 0.569, the recall is 0.57. According to the learning curve, the model is not overfitting.
6. Then we established the LightGBM machine learning algorithm using grid search to find the best model based on different hyperparameter. The result shows the validation mean accuracy is 0.557 and the AUC is 0.580. The test data shows accuracy=0.57, AUC=0.580. Based on the learning curve, it shows the data is overfitting.

**5. Classification (Deep Learning based)** \
*5.1 Process* \
1. We use deep learning neural network, including layer1(32), layer2(16), dense(1). epoach is 10, batch size is 16. The AUC is 0.5504, accuracy is 0.55. The reason why the performance is worse than tree-based model. Because the dataset might include some information about non-linear.
2. According to learning curve, the Training loss is decreasing while test loss is decresing, which means the model is not overfit.
3. We use TensorBoard to visulize the training process.

**6. Stacked Ensemble Model** \
*6.1 Process* \
1. Stack KNN,RF,LR,XGB,LGB. The AUC is 0.5589, accuracy is 0.56.
2. Stack XGB,LGB. The AUC is 0.5271, accuracy is 0.53.
3. Stack ensemble, a combination of multiple machine learning models, offers several benefits. It can improve predictive accuracy by leveraging the strengths of individual models, reduce overfitting, and handle complex relationships in data. By blending diverse models, it can capture different patterns and provide robust predictions. It also enables flexibility in model selection and can adapt to various types of data and problem domains.

**7. Explainable AI** \
1. Feature Importance: According to L1 & L2, The most important feature is Census_ActivationChannel, Processor,Census_InternalPrimaryDisplayResolutionHorizontal, Census_InternalPrimaryDiagonalDisplaySizeinches.
2. Feature Importance: According to XGBoost, the most important feature is AVProductStatesIdentifier, Census_InternalPrimaryDiagonalDisplaySizeinches, Census_TotalPhysicalRAM.
3. Shapley Value: Census_PrimaryDiskTotalCapacity, AVProductStatesIdentifier, Processor.
4. Partial Dependence Plot: AVProductStatesIdentifier and Census_PrimaryDiskTotalCapacity have obvious change with target.
5. Surrogate Model(Tree): Census_PrimaryDiskTotalCapacity and Census_OSArchitecture and Census_FlightRing is most important.

**8. Winner** \

Classifier performance results are as follows (results may vary due to random seed). Ordered by AUC.

|**Classifier** |    Accuracy    |       AUC      |    Recall    |
|:----------------------|:------------:|:--------------:|:-------------:|
|**LightGBM(mycode)** | **0.57** | **0.5797** | **0.56** |
|Naive Bayes | 0.5571 | 0.5784 |0.8462 | 
|Gradient Boosing Classifier | 0.5514 | 0.5764 | 0.6274 |
|Ada Boost Classifier | 0.5505 | 0.5704 |0.6226 |
|XGBoost(mycode) | 0.57 | 0.5689 | 0.57 |
|Linear Discriminant Analysis | 0.5548 | 0.5680 | 0.6104 |
|Stack(KNN,RF,LR,XGB,LGB) | 0.56 | 0.5590 | 0.56 |
|Deep Neural Network(mycode) | 0.55 | 0.5505 | 0.55 |
|LightGBM | 0.5319 | 0.5426 | 0.5396 |
|Random Forest | 0.5314 |  0.5416 | 0.5642 |
|Logistic Regression | 0.5248 | 0.5388 | 0.6283 |
|XGBoost | 0.5228 | 0.5283 | 0.5443 |
|Logistic Regression(mycode) | 0.5283 | 0.5272 | 0.53 |
|Stack(XGB,LGB) | 0.53 | 0.5271 | 0.53 |
|Extra Trees Classifier | 0.5148 | 0.5211 | 0.5330 | 
|SVM | 0.4914 | 0 | 0.3953 |




![image](https://github.com/muhaochen-2021/Multi-Factor-Selection-Mode/blob/main/0.PNG)
